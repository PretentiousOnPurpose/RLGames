{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from board import Board\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ['current_state', 'current_action', 'next_state', 'reward', 'done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentConfig = {'epsilon': 0.7, 'step_size': 0.4, 'gamma': 0.999}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.pos = 0\n",
    "    \n",
    "    def push(self, current_state, current_action, next_state, reward, done):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        \n",
    "        self.memory[self.pos] = Transition(current_state, current_action, next_state, reward, done)\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        sample_idx = np.random.choice(np.linspace(0, self.capacity - 1, self.capacity, dtype=int), batch_size)\n",
    "\n",
    "        x_cs = np.zeros((batch_size, 9))\n",
    "        x_ns = np.zeros((batch_size, 9))\n",
    "        a_t = np.zeros((batch_size, 1))\n",
    "        r = np.zeros((batch_size, 1))  \n",
    "        d = np.zeros((batch_size, 1))  \n",
    "        \n",
    "        for iter_tr in range(batch_size):\n",
    "            x_cs[iter_tr, :] = self.memory[iter_tr][0].reshape(-1, 9)\n",
    "            x_ns[iter_tr, :] = self.memory[iter_tr][2].reshape(-1, 9)\n",
    "            a_t[iter_tr, :] = np.array([self.memory[iter_tr][1][0] * 3 + self.memory[iter_tr][1][1]], dtype=int).reshape(-1, 1)\n",
    "            r[iter_tr, :] = np.array(self.memory[iter_tr][3]).reshape(-1, 1)\n",
    "            d[iter_tr, :] = np.array(self.memory[iter_tr][4]).reshape(-1, 1)\n",
    "        \n",
    "        return [x_cs, a_t, x_ns, r, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 27)\n",
    "        self.fc2 = nn.Linear(27, 27)\n",
    "        self.fc3 = nn.Linear(27, 27)\n",
    "        self.fc4 = nn.Linear(27, 9)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y1 = self.fc1(x)\n",
    "#         y1 = F.relu(y1)\n",
    "        \n",
    "        y2 = self.fc2(y1)\n",
    "#         y2 = F.relu(y2)\n",
    "        \n",
    "        y3 = self.fc3(y2)\n",
    "#         y3 = F.relu(y3)\n",
    "        \n",
    "        y4 = self.fc4(y3)\n",
    "\n",
    "        return y4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(current_state, Q, ValidActionSpace, agentConfig):\n",
    "    action_values = Q.forward(torch.from_numpy(current_state.reshape(-1, 9)).float().cuda()).cpu().detach().numpy().reshape(-1,)\n",
    "    valid_actions = ValidActionSpace[:, 0] * 3 + ValidActionSpace[:, 1]\n",
    "    \n",
    "    \n",
    "    pr = np.random.rand(1)[0]\n",
    "    \n",
    "    if pr <= 1 - agentConfig['epsilon']:\n",
    "        next_action_idx = np.argmax(action_values[valid_actions])\n",
    "        next_action = valid_actions[next_action_idx]\n",
    "    else:\n",
    "        next_action_idx = np.random.choice(np.linspace(0, len(valid_actions) - 1, len(valid_actions)))\n",
    "        next_action = valid_actions[int(next_action_idx)]\n",
    "\n",
    "    return [int(next_action / 3), next_action % 3]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(numEpisodes = 500000):\n",
    "    replayMemory = ExperienceReplay(50000)\n",
    "    \n",
    "    env = Board()\n",
    "    env.reset()\n",
    "        \n",
    "    for iter_episodes in range(numEpisodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        \n",
    "        if iter_episodes <= 50000:\n",
    "            agentConfig['epsilon'] = 0.9\n",
    "        elif iter_episodes <= 60000:\n",
    "            agentConfig['epsilon'] = 0.5\n",
    "        elif iter_episodes <= 70000:\n",
    "            agentConfig['epsilon'] = 0.5\n",
    "        else:\n",
    "            agentConfig['epsilon'] = 0.1\n",
    "        \n",
    "        while not done:\n",
    "            current_state1 = np.copy(env.current_state)\n",
    "            action_space = env.getValidActionSpace()\n",
    "            current_action1 = select_action(current_state1, policy_net, action_space, agentConfig)\n",
    "            [next_state1, reward1, done] = env.step(current_action1, 1)\n",
    "            replayMemory.push(current_state1, current_action1, next_state1, reward1, done)\n",
    "            \n",
    "            if not done:\n",
    "                current_state2 = np.copy(next_state1) * -1\n",
    "                action_space = env.getValidActionSpace()\n",
    "                current_action2 = select_action(current_state2, policy_net, action_space, agentConfig)\n",
    "                [next_state2, reward2, done] = env.step(current_action2, -1)\n",
    "                replayMemory.push(current_state2, current_action2, next_state2 * -1, reward2, done)\n",
    "            \n",
    "        if (iter_episodes + 1) % 10000 == 0:\n",
    "            loss_m = 0\n",
    "            for iter_batches in range(100):\n",
    "                [x_cs, a_t, x_ns, r, d] = replayMemory.sample(1000)\n",
    "                q_cs = policy_net.forward(torch.from_numpy(x_cs).float().cuda()).cpu().reshape(-1, 9)\n",
    "                q_ns = target_net.forward(torch.from_numpy(x_ns).float().cuda()).cpu().reshape(-1, 9)\n",
    "                r = torch.from_numpy(r).float()\n",
    "                a_t = torch.from_numpy(a_t)\n",
    "                d = torch.from_numpy(d).float()\n",
    "\n",
    "                estimate = torch.stack([q[int(i[0].numpy())] for q, i in zip(q_cs, a_t)]).reshape(-1, 1)\n",
    "                target = r + agentConfig['gamma'] * torch.argmax(q_ns, axis = 1).float().reshape(-1, 1) * (1 - d)\n",
    "                \n",
    "                loss = F.smooth_l1_loss(estimate, target)\n",
    "                \n",
    "                loss_m += loss.item()\n",
    "                \n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                for param in policy_net.parameters():\n",
    "                    param.grad.data.clamp_(-1, 1)\n",
    "                \n",
    "                if (iter_batches + 1) % 25 == 0:\n",
    "                    target_net.load_state_dict(policy_net.state_dict())\n",
    "                opt.step()   \n",
    "\n",
    "            now = datetime.now()\n",
    "            print(\"Loss: \", loss_m / 100, \" | Time: \", now.strftime(\"%H:%M:%S\"))\n",
    "            torch.save(policy_net.state_dict(), 'policy_net.pt')\n",
    "            torch.save(target_net.state_dict(), 'target_net.pt')\n",
    "            torch.save(opt.state_dict(), 'opt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN().cuda()\n",
    "target_net = DQN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(params = policy_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  1.982891103029251  | Time:  20:25:08\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
