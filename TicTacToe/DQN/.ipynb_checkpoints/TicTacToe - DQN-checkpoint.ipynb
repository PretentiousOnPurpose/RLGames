{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from board import Board\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ['current_state', 'current_action', 'next_state', 'reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentConfig = {'epsilon': 0.7, 'step_size': 0.4, 'gamma': 0.999}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.pos = 0\n",
    "    \n",
    "    def push(self, current_state, current_action, next_state, reward):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        \n",
    "        self.memory[self.pos] = Transition(current_state, current_action, next_state, reward)\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        sample_idx = np.random.choice(np.linspace(0, self.capacity - 1, self.capacity, dtype=int), batch_size)\n",
    "\n",
    "        x_cs = np.zeros((batch_size, 9))\n",
    "        x_ns = np.zeros((batch_size, 9))\n",
    "        a_t = np.zeros((batch_size, 1))\n",
    "        r = np.zeros((batch_size, 1))  \n",
    "        \n",
    "        for iter_tr in range(batch_size):\n",
    "            x_cs[iter_tr, :] = self.memory[iter_tr][0].reshape(-1, 9)\n",
    "            x_ns[iter_tr, :] = self.memory[iter_tr][2].reshape(-1, 9)\n",
    "            a_t[iter_tr, :] = np.array([self.memory[iter_tr][1][0] * 3 + self.memory[iter_tr][1][1]], dtype=int).reshape(-1, 1)\n",
    "            r[iter_tr, :] = np.array(self.memory[iter_tr][3]).reshape(-1, 1)\n",
    "        \n",
    "        \n",
    "        return [x_cs, a_t, x_ns, r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 9)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y1 = self.fc1(x)\n",
    "        y1 = F.relu(y1)\n",
    "        \n",
    "        y2 = self.fc2(y1)\n",
    "        y2 = F.relu(y2)\n",
    "        \n",
    "        y3 = self.fc3(y2)\n",
    "        \n",
    "        return y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(current_state, Q, ValidActionSpace, agentConfig):\n",
    "    action_values = Q.forward(torch.from_numpy(current_state.reshape(-1, 9)).float()).detach().numpy().reshape(-1,)\n",
    "    valid_actions = ValidActionSpace[:, 0] * 3 + ValidActionSpace[:, 1]\n",
    "    \n",
    "    \n",
    "    pr = np.random.rand(1)[0]\n",
    "    \n",
    "    if pr <= 1 - agentConfig['epsilon']:\n",
    "        next_action_idx = np.argmax(action_values[valid_actions])\n",
    "        next_action = valid_actions[next_action_idx]\n",
    "    else:\n",
    "        next_action_idx = np.random.choice(np.linspace(0, len(valid_actions) - 1, len(valid_actions)))\n",
    "        next_action = valid_actions[int(next_action_idx)]\n",
    "\n",
    "    return [int(next_action / 3), next_action % 3]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(numEpisodes = 100000):\n",
    "    replayMemory = ExperienceReplay(5000)\n",
    "    \n",
    "    env = Board()\n",
    "    env.reset()\n",
    "        \n",
    "    for iter_episodes in range(numEpisodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        \n",
    "        if iter_episodes <= 15000:\n",
    "            agentConfig['epsilon'] = 0.75\n",
    "        elif iter_episodes <= 25000:\n",
    "            agentConfig['epsilon'] = 0.5\n",
    "        elif iter_episodes <= 45000:\n",
    "            agentConfig['epsilon'] = 0.3\n",
    "        else:\n",
    "            agentConfig['epsilon'] = 0.1\n",
    "        \n",
    "        while not done:\n",
    "            current_state1 = np.copy(env.current_state)\n",
    "            action_space = env.getValidActionSpace()\n",
    "            current_action1 = select_action(current_state1, policy_net, action_space, agentConfig)\n",
    "            [next_state1, reward1, done] = env.step(current_action1, 1)\n",
    "            replayMemory.push(current_state1, current_action1, next_state1, reward1)\n",
    "            \n",
    "            if not done:\n",
    "                current_state2 = np.copy(next_state1) * -1\n",
    "                action_space = env.getValidActionSpace()\n",
    "                current_action2 = select_action(current_state2, policy_net, action_space, agentConfig)\n",
    "                [next_state2, reward2, done] = env.step(current_action2, -1)\n",
    "                replayMemory.push(current_state2, current_action2, next_state2, reward2)\n",
    "            \n",
    "        if (iter_episodes + 1) % 1000 == 0:\n",
    "            for iter_batches in range(50):\n",
    "                [x_cs, a_t, x_ns, r] = replayMemory.sample(100)\n",
    "                q_cs = policy_net.forward(torch.from_numpy(x_cs).float()).reshape(-1, 9)\n",
    "                q_ns = target_net.forward(torch.from_numpy(x_ns).float()).reshape(-1, 9)\n",
    "                r = torch.from_numpy(r).float()\n",
    "                a_t = torch.from_numpy(a_t)\n",
    "\n",
    "                estimate = torch.stack([q[int(i[0].numpy())] for q, i in zip(q_cs, a_t)]).reshape(-1, 1)\n",
    "                target = r + agentConfig['gamma'] * torch.argmax(q_ns, axis = 1).reshape(-1, 1)\n",
    "                \n",
    "                loss = nn.MSELoss()(estimate, target)\n",
    "                \n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                for param in policy_net.parameters():\n",
    "                    param.grad.data.clamp_(-1, 1)\n",
    "                \n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "                opt.step()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN()\n",
    "target_net = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(params = policy_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = torch.stack([q[int(i[0].numpy())] for q, i in zip(q_cs, a_t)]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1160, -0.0367, -0.1041,  0.0303,  0.2217,  0.1554,  0.0280,  0.0539,\n",
       "        -0.1319], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_cs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1., -1.,  0.,  1.,  1., -1.,  0., -1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-0., -0., -0., -0., -0., -0., -1., -0., -0.],\n",
       "       [ 0.,  0.,  0., -1.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [-0., -0., -0.,  1., -1., -0., -1., -0., -0.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit33c1cdda75ee49ec90b005d9162fae6b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
